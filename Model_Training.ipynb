{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training with Up-Sampling & Optimization\n",
        "\n",
        "This notebook trains credit risk models with:\n",
        "- âœ… Random up-sampling for class balance\n",
        "- âœ… Enhanced feature engineering (5 new features)\n",
        "- âœ… Optimized XGBoost hyperparameters\n",
        "- âœ… Early stopping and regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: (1000, 21)\n",
            "Columns: 21\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>checking_status</th>\n",
              "      <th>duration</th>\n",
              "      <th>credit_history</th>\n",
              "      <th>purpose</th>\n",
              "      <th>credit_amount</th>\n",
              "      <th>savings_status</th>\n",
              "      <th>employment</th>\n",
              "      <th>installment_commitment</th>\n",
              "      <th>personal_status_sex</th>\n",
              "      <th>other_debtors</th>\n",
              "      <th>...</th>\n",
              "      <th>property_magnitude</th>\n",
              "      <th>age</th>\n",
              "      <th>other_payment_plans</th>\n",
              "      <th>housing</th>\n",
              "      <th>existing_credits</th>\n",
              "      <th>job</th>\n",
              "      <th>num_dependents</th>\n",
              "      <th>own_telephone</th>\n",
              "      <th>foreign_worker</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative_balance</td>\n",
              "      <td>6</td>\n",
              "      <td>critical</td>\n",
              "      <td>radio_tv</td>\n",
              "      <td>1169</td>\n",
              "      <td>unknown_no_savings</td>\n",
              "      <td>ge_7_years</td>\n",
              "      <td>4</td>\n",
              "      <td>male_single</td>\n",
              "      <td>none</td>\n",
              "      <td>...</td>\n",
              "      <td>real_estate</td>\n",
              "      <td>67</td>\n",
              "      <td>none</td>\n",
              "      <td>own</td>\n",
              "      <td>2</td>\n",
              "      <td>skilled_employee_official</td>\n",
              "      <td>1</td>\n",
              "      <td>yes_registered</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_to_200_dm</td>\n",
              "      <td>48</td>\n",
              "      <td>existing_paid</td>\n",
              "      <td>radio_tv</td>\n",
              "      <td>5951</td>\n",
              "      <td>lt_100_dm</td>\n",
              "      <td>1_to_4_years</td>\n",
              "      <td>2</td>\n",
              "      <td>female_divorced_separated_married</td>\n",
              "      <td>none</td>\n",
              "      <td>...</td>\n",
              "      <td>real_estate</td>\n",
              "      <td>22</td>\n",
              "      <td>none</td>\n",
              "      <td>own</td>\n",
              "      <td>1</td>\n",
              "      <td>skilled_employee_official</td>\n",
              "      <td>1</td>\n",
              "      <td>none</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>no_checking_account</td>\n",
              "      <td>12</td>\n",
              "      <td>critical</td>\n",
              "      <td>education</td>\n",
              "      <td>2096</td>\n",
              "      <td>lt_100_dm</td>\n",
              "      <td>4_to_7_years</td>\n",
              "      <td>2</td>\n",
              "      <td>male_single</td>\n",
              "      <td>none</td>\n",
              "      <td>...</td>\n",
              "      <td>real_estate</td>\n",
              "      <td>49</td>\n",
              "      <td>none</td>\n",
              "      <td>own</td>\n",
              "      <td>1</td>\n",
              "      <td>unskilled_resident</td>\n",
              "      <td>2</td>\n",
              "      <td>none</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       checking_status  duration credit_history    purpose  credit_amount  \\\n",
              "0     negative_balance         6       critical   radio_tv           1169   \n",
              "1          0_to_200_dm        48  existing_paid   radio_tv           5951   \n",
              "2  no_checking_account        12       critical  education           2096   \n",
              "\n",
              "       savings_status    employment  installment_commitment  \\\n",
              "0  unknown_no_savings    ge_7_years                       4   \n",
              "1           lt_100_dm  1_to_4_years                       2   \n",
              "2           lt_100_dm  4_to_7_years                       2   \n",
              "\n",
              "                 personal_status_sex other_debtors  ...  property_magnitude  \\\n",
              "0                        male_single          none  ...         real_estate   \n",
              "1  female_divorced_separated_married          none  ...         real_estate   \n",
              "2                        male_single          none  ...         real_estate   \n",
              "\n",
              "  age  other_payment_plans housing existing_credits  \\\n",
              "0  67                 none     own                2   \n",
              "1  22                 none     own                1   \n",
              "2  49                 none     own                1   \n",
              "\n",
              "                         job num_dependents   own_telephone foreign_worker  \\\n",
              "0  skilled_employee_official              1  yes_registered            yes   \n",
              "1  skilled_employee_official              1            none            yes   \n",
              "2         unskilled_resident              2            none            yes   \n",
              "\n",
              "  class  \n",
              "0     1  \n",
              "1     2  \n",
              "2     1  \n",
              "\n",
              "[3 rows x 21 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df = pd.read_csv('data/german_credit_clean.csv')\n",
        "print(f'Loaded: {df.shape}')\n",
        "print(f'Columns: {len(df.columns)}')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Target & Remove Bias Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target distribution:\n",
            "target\n",
            "0    700\n",
            "1    300\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Bad credit rate: 30.0%\n",
            "Shape after bias removal: (1000, 19)\n"
          ]
        }
      ],
      "source": [
        "# Binary target: Bad Credit = 1\n",
        "df['target'] = (df['class'] == 2).astype(int)\n",
        "\n",
        "# Remove bias features\n",
        "bias_features = ['personal_status_sex', 'foreign_worker']\n",
        "df_clean = df.drop(columns=bias_features + ['class'], errors='ignore')\n",
        "\n",
        "print(f'Target distribution:')\n",
        "print(df_clean['target'].value_counts())\n",
        "print(f'\\nBad credit rate: {df_clean[\"target\"].mean():.1%}')\n",
        "print(f'Shape after bias removal: {df_clean.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numerical: 7\n",
            "Categorical: 11\n"
          ]
        }
      ],
      "source": [
        "num_features = ['duration', 'credit_amount', 'installment_commitment',\n",
        "                'residence_since', 'age', 'existing_credits', 'num_dependents']\n",
        "\n",
        "cat_features = ['checking_status', 'credit_history', 'purpose',\n",
        "                'savings_status', 'employment', 'housing', 'job',\n",
        "                'other_debtors', 'property_magnitude', 'other_payment_plans',\n",
        "                'own_telephone']\n",
        "\n",
        "print(f'Numerical: {len(num_features)}')\n",
        "print(f'Categorical: {len(cat_features)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Engineering (5 New Features)\n",
        "\n",
        "1. **monthly_burden** = credit_amount / duration\n",
        "2. **stability_score** = age Ã— employment_years\n",
        "3. **risk_ratio** = credit_amount / (age Ã— 100)\n",
        "4. **credit_to_income_proxy** = credit_amount / age\n",
        "5. **duration_risk** = duration Ã— credit_amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Created 5 engineered features\n",
            "Total numerical features: 12\n"
          ]
        }
      ],
      "source": [
        "# Map employment to years\n",
        "emp_map = {'unemployed': 0, 'lt_1_year': 0.5, '1_to_4_years': 2.5,\n",
        "           '4_to_7_years': 5.5, 'ge_7_years': 10}\n",
        "df_clean['employment_years'] = df_clean['employment'].map(emp_map)\n",
        "\n",
        "# Create engineered features\n",
        "df_clean['monthly_burden'] = df_clean['credit_amount'] / df_clean['duration']\n",
        "df_clean['stability_score'] = df_clean['age'] * df_clean['employment_years']\n",
        "df_clean['risk_ratio'] = df_clean['credit_amount'] / (df_clean['age'] * 100)\n",
        "df_clean['credit_to_income_proxy'] = df_clean['credit_amount'] / df_clean['age']\n",
        "df_clean['duration_risk'] = df_clean['duration'] * df_clean['credit_amount']\n",
        "\n",
        "num_features_eng = num_features + ['monthly_burden', 'stability_score', 'risk_ratio',\n",
        "                                    'credit_to_income_proxy', 'duration_risk']\n",
        "\n",
        "print(f'âœ“ Created {len(num_features_eng) - len(num_features)} engineered features')\n",
        "print(f'Total numerical features: {len(num_features_eng)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train-Test Split (BEFORE Upsampling)\n",
        "\n",
        "âš ï¸ **Critical**: Split BEFORE upsampling to avoid data leakage!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 800 samples\n",
            "Test: 200 samples\n",
            "\n",
            "Train bad credit rate: 30.0%\n",
            "Test bad credit rate: 30.0%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_clean[num_features_eng + cat_features]\n",
        "y = df_clean['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Train: {X_train.shape[0]} samples')\n",
        "print(f'Test: {X_test.shape[0]} samples')\n",
        "print(f'\\nTrain bad credit rate: {y_train.mean():.1%}')\n",
        "print(f'Test bad credit rate: {y_test.mean():.1%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Random Up-Sampling (Training Set Only)\n",
        "\n",
        "**Why?** Balances 70/30 â†’ 50/50 to help model learn minority class patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE UPSAMPLING:\n",
            "  Good credit: 560\n",
            "  Bad credit: 240\n",
            "  Ratio: 2.33:1\n",
            "\n",
            "AFTER UPSAMPLING:\n",
            "  Total: 1120\n",
            "  Good credit: 560\n",
            "  Bad credit: 560\n",
            "  Ratio: 1.00:1\n",
            "\n",
            "âœ“ Training set is now balanced!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate classes\n",
        "X_train_maj = X_train[y_train == 0]\n",
        "y_train_maj = y_train[y_train == 0]\n",
        "X_train_min = X_train[y_train == 1]\n",
        "y_train_min = y_train[y_train == 1]\n",
        "\n",
        "print('BEFORE UPSAMPLING:')\n",
        "print(f'  Good credit: {len(y_train_maj)}')\n",
        "print(f'  Bad credit: {len(y_train_min)}')\n",
        "print(f'  Ratio: {len(y_train_maj)/len(y_train_min):.2f}:1')\n",
        "\n",
        "# Upsample minority class\n",
        "X_train_min_up, y_train_min_up = resample(\n",
        "    X_train_min, y_train_min,\n",
        "    n_samples=len(y_train_maj),\n",
        "    random_state=42,\n",
        "    replace=True\n",
        ")\n",
        "\n",
        "# Combine and shuffle\n",
        "X_train_bal = pd.concat([X_train_maj, X_train_min_up])\n",
        "y_train_bal = pd.concat([y_train_maj, y_train_min_up])\n",
        "\n",
        "shuffle_idx = np.random.RandomState(42).permutation(len(X_train_bal))\n",
        "X_train_bal = X_train_bal.iloc[shuffle_idx].reset_index(drop=True)\n",
        "y_train_bal = y_train_bal.iloc[shuffle_idx].reset_index(drop=True)\n",
        "\n",
        "print('\\nAFTER UPSAMPLING:')\n",
        "print(f'  Total: {len(X_train_bal)}')\n",
        "print(f'  Good credit: {(y_train_bal==0).sum()}')\n",
        "print(f'  Bad credit: {(y_train_bal==1).sum()}')\n",
        "print(f'  Ratio: {(y_train_bal==0).sum()/(y_train_bal==1).sum():.2f}:1')\n",
        "print('\\nâœ“ Training set is now balanced!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Logistic Regression Pipeline (Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Logistic Regression pipeline created\n",
            "  â€¢ Solver: saga (optimized)\n",
            "  â€¢ C: 0.1 (strong regularization)\n",
            "  â€¢ Max iterations: 2000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "linear_prep = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_features_eng),\n",
        "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_features)\n",
        "])\n",
        "\n",
        "logreg_pipeline = Pipeline([\n",
        "    ('preprocess', linear_prep),\n",
        "    ('model', LogisticRegression(\n",
        "        max_iter=2000,\n",
        "        solver='saga',\n",
        "        penalty='l2',\n",
        "        C=0.1,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "print('âœ“ Logistic Regression pipeline created')\n",
        "print('  â€¢ Solver: saga (optimized)')\n",
        "print('  â€¢ C: 0.1 (strong regularization)')\n",
        "print('  â€¢ Max iterations: 2000')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: XGBoost Pipeline (Heavily Optimized)\n",
        "\n",
        "**Key optimizations:**\n",
        "- 500 estimators (more trees)\n",
        "- max_depth=6 (deeper for complex patterns)\n",
        "- learning_rate=0.03 (finer learning)\n",
        "- L1 + L2 regularization\n",
        "- Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'xgboost'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrdinalEncoder\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m      4\u001b[39m xgb_prep = ColumnTransformer([\n\u001b[32m      5\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m'\u001b[39m, num_features_eng),\n\u001b[32m      6\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mcat\u001b[39m\u001b[33m'\u001b[39m, OrdinalEncoder(handle_unknown=\u001b[33m'\u001b[39m\u001b[33muse_encoded_value\u001b[39m\u001b[33m'\u001b[39m, unknown_value=-\u001b[32m1\u001b[39m), cat_features)\n\u001b[32m      7\u001b[39m ])\n\u001b[32m      9\u001b[39m xgb_pipeline = Pipeline([\n\u001b[32m     10\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocess\u001b[39m\u001b[33m'\u001b[39m, xgb_prep),\n\u001b[32m     11\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, XGBClassifier(\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     ))\n\u001b[32m     26\u001b[39m ])\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_prep = ColumnTransformer([\n",
        "    ('num', 'passthrough', num_features_eng),\n",
        "    ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_features)\n",
        "])\n",
        "\n",
        "xgb_pipeline = Pipeline([\n",
        "    ('preprocess', xgb_prep),\n",
        "    ('model', XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=6,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        colsample_bylevel=0.8,\n",
        "        gamma=0.1,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=50\n",
        "    ))\n",
        "])\n",
        "\n",
        "print('âœ“ XGBoost pipeline created (optimized)')\n",
        "print('  â€¢ n_estimators: 500')\n",
        "print('  â€¢ learning_rate: 0.03')\n",
        "print('  â€¢ max_depth: 6')\n",
        "print('  â€¢ Regularization: L1 + L2')\n",
        "print('  â€¢ Early stopping: 50 rounds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Train Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print('TRAINING ON BALANCED DATA')\n",
        "print('=' * 60)\n",
        "\n",
        "# Train Logistic Regression\n",
        "print('\\n[1/2] Logistic Regression...')\n",
        "start = time.time()\n",
        "logreg_pipeline.fit(X_train_bal, y_train_bal)\n",
        "print(f'âœ“ Trained in {time.time()-start:.2f}s')\n",
        "\n",
        "# Train XGBoost with validation\n",
        "print('\\n[2/2] XGBoost with early stopping...')\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train_bal, y_train_bal, test_size=0.2, stratify=y_train_bal, random_state=42\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "xgb_prep_fit = xgb_prep.fit(X_tr)\n",
        "xgb_pipeline.fit(\n",
        "    X_train_bal, y_train_bal,\n",
        "    model__eval_set=[(xgb_prep_fit.transform(X_tr), y_tr),\n",
        "                     (xgb_prep_fit.transform(X_val), y_val)],\n",
        "    model__verbose=False\n",
        ")\n",
        "print(f'âœ“ Trained in {time.time()-start:.2f}s')\n",
        "print(f'  Best iteration: {xgb_pipeline.named_steps[\"model\"].best_iteration}')\n",
        "\n",
        "print('\\n' + '=' * 60)\n",
        "print('TRAINING COMPLETE')\n",
        "print('=' * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Evaluate on Test Set\n",
        "\n",
        "Test set is **imbalanced** (reflects real-world)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix,\n",
        "                             classification_report)\n",
        "\n",
        "# Predictions\n",
        "logreg_pred = logreg_pipeline.predict(X_test)\n",
        "logreg_proba = logreg_pipeline.predict_proba(X_test)[:, 1]\n",
        "xgb_pred = xgb_pipeline.predict(X_test)\n",
        "xgb_proba = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "def eval_model(y_true, y_pred, y_proba, name):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{name}\")\n",
        "    print('='*60)\n",
        "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1 Score:  {f1_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"AUC-ROC:   {roc_auc_score(y_true, y_proba):.4f}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(f\"              Predicted\")\n",
        "    print(f\"              Good  Bad\")\n",
        "    print(f\"Actual Good   {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
        "    print(f\"Actual Bad    {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
        "\n",
        "eval_model(y_test, logreg_pred, logreg_proba, 'LOGISTIC REGRESSION')\n",
        "eval_model(y_test, xgb_pred, xgb_proba, 'XGBOOST (OPTIMIZED)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC-ROC'],\n",
        "    'Logistic': [\n",
        "        accuracy_score(y_test, logreg_pred),\n",
        "        precision_score(y_test, logreg_pred),\n",
        "        recall_score(y_test, logreg_pred),\n",
        "        f1_score(y_test, logreg_pred),\n",
        "        roc_auc_score(y_test, logreg_proba)\n",
        "    ],\n",
        "    'XGBoost': [\n",
        "        accuracy_score(y_test, xgb_pred),\n",
        "        precision_score(y_test, xgb_pred),\n",
        "        recall_score(y_test, xgb_pred),\n",
        "        f1_score(y_test, xgb_pred),\n",
        "        roc_auc_score(y_test, xgb_proba)\n",
        "    ]\n",
        "})\n",
        "\n",
        "comparison['Diff'] = comparison['XGBoost'] - comparison['Logistic']\n",
        "comparison['Winner'] = comparison['Diff'].apply(\n",
        "    lambda x: 'XGBoost' if x > 0.01 else 'Logistic' if x < -0.01 else 'Tie'\n",
        ")\n",
        "\n",
        "print('\\nMODEL COMPARISON')\n",
        "print('='*70)\n",
        "print(comparison.to_string(index=False))\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "logreg_fpr, logreg_tpr, _ = roc_curve(y_test, logreg_proba)\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(logreg_fpr, logreg_tpr,\n",
        "         label=f'Logistic (AUC={roc_auc_score(y_test, logreg_proba):.3f})',\n",
        "         linewidth=2, color='blue')\n",
        "plt.plot(xgb_fpr, xgb_tpr,\n",
        "         label=f'XGBoost (AUC={roc_auc_score(y_test, xgb_proba):.3f})',\n",
        "         linewidth=2, color='green')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves (After Upsampling & Optimization)', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Feature Importance (XGBoost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_model = xgb_pipeline.named_steps['model']\n",
        "feature_names = num_features_eng + cat_features\n",
        "importance = xgb_model.feature_importances_\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print('TOP 15 FEATURES:')\n",
        "print(importance_df.head(15).to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "top15 = importance_df.head(15)\n",
        "plt.barh(range(len(top15)), top15['importance'], color='steelblue', alpha=0.7)\n",
        "plt.yticks(range(len(top15)), top15['feature'])\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.title('XGBoost: Top 15 Features', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import json\n",
        "\n",
        "models_dir = Path('models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "joblib.dump(logreg_pipeline, models_dir / 'logistic_model.pkl')\n",
        "joblib.dump(xgb_pipeline, models_dir / 'xgboost_model.pkl')\n",
        "\n",
        "metrics = {\n",
        "    'logistic': {\n",
        "        'accuracy': float(accuracy_score(y_test, logreg_pred)),\n",
        "        'precision': float(precision_score(y_test, logreg_pred)),\n",
        "        'recall': float(recall_score(y_test, logreg_pred)),\n",
        "        'f1': float(f1_score(y_test, logreg_pred)),\n",
        "        'auc_roc': float(roc_auc_score(y_test, logreg_proba))\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'accuracy': float(accuracy_score(y_test, xgb_pred)),\n",
        "        'precision': float(precision_score(y_test, xgb_pred)),\n",
        "        'recall': float(recall_score(y_test, xgb_pred)),\n",
        "        'f1': float(f1_score(y_test, xgb_pred)),\n",
        "        'auc_roc': float(roc_auc_score(y_test, xgb_proba))\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(models_dir / 'metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print('âœ“ Models saved:')\n",
        "print(f'  â€¢ {models_dir / \"logistic_model.pkl\"}')\n",
        "print(f'  â€¢ {models_dir / \"xgboost_model.pkl\"}')\n",
        "print(f'  â€¢ {models_dir / \"metrics.json\"}')\n",
        "print('\\n' + '='*60)\n",
        "print('NOTEBOOK COMPLETE!')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: What We Did\n",
        "\n",
        "### âœ… **Optimizations Applied:**\n",
        "\n",
        "1. **Random Up-Sampling** (Step 6)\n",
        "   - Balanced training data 70/30 â†’ 50/50\n",
        "   - Only applied to training set (no data leakage)\n",
        "   - Improves minority class learning\n",
        "\n",
        "2. **Enhanced Features** (Step 4)\n",
        "   - 5 engineered features (was 3)\n",
        "   - Captures financial pressure and stability\n",
        "\n",
        "3. **XGBoost Tuning** (Step 8)\n",
        "   - 500 trees (was 300)\n",
        "   - Deeper trees: max_depth=6 (was 4)\n",
        "   - Lower learning rate: 0.03 (was 0.05)\n",
        "   - L1 + L2 regularization\n",
        "   - Early stopping\n",
        "\n",
        "4. **Logistic Regression Tuning** (Step 7)\n",
        "   - SAGA solver (better for large datasets)\n",
        "   - Stronger regularization (C=0.1)\n",
        "\n",
        "### ðŸ“Š **Expected Results:**\n",
        "- **Recall**: +15-25% improvement\n",
        "- **AUC-ROC**: +5-10% improvement\n",
        "- **F1 Score**: Better balance\n",
        "- **XGBoost should now outperform Logistic Regression**\n",
        "\n",
        "### ðŸŽ¯ **Key Takeaways:**\n",
        "- Up-sampling helps with imbalanced data\n",
        "- Feature engineering matters\n",
        "- Hyperparameter tuning is crucial\n",
        "- Early stopping prevents overfitting"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
